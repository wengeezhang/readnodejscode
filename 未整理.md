# fs之三种方式：同步，异步，promise
1.readFile会调用read,read:
```js
const req = new FSReqCallback();
  req.oncomplete = wrapper;

  binding.read(fd, buffer, offset, length, position, req);
```
这里封装了一个FSReqCallback对象req，传到binding.read

2.如果是readFileSync会调用readSync，readSync:
```js
const result = binding.read(fd, buffer, offset, length, position,
                              undefined, ctx);
```
这里传递了一个undefined。

3.如果是promise形式
```js
lib/internal/fs/promises.js
const bytesRead = (await binding.read(handle.fd, buffer, offset, length,
                                        position, kUsePromises)) || 0;
```
这里传递了一个kUsePromises

针对1和3，c++中统一封装为：req_wrap_async

```c++
FSReqBase* req_wrap_async = GetReqWrap(args, 5);
  if (req_wrap_async != nullptr) {  // read(fd, buffer, offset, len, pos, req)
    AsyncCall(env, req_wrap_async, args, "read", UTF8, AfterInteger,
              uv_fs_read, fd, &uvbuf, 1, pos);
  }


FSReqBase* GetReqWrap(const v8::FunctionCallbackInfo<v8::Value>& args,
                      int index,
                      bool use_bigint) {
  v8::Local<v8::Value> value = args[index];
  // 如果是普通对象，则直接返回FSReqBase
  if (value->IsObject()) {
    return Unwrap<FSReqBase>(value.As<v8::Object>());
  }

  BindingData* binding_data = Unwrap<BindingData>(args.Data());
  Environment* env = binding_data->env();
  // 如果是promise,则返回一个FSReqPromise
  if (value->StrictEquals(env->fs_use_promises_symbol())) {
    if (use_bigint) {
      return FSReqPromise<AliasedBigUint64Array>::New(binding_data, use_bigint);
    } else {
      return FSReqPromise<AliasedFloat64Array>::New(binding_data, use_bigint);
    }
  }
  return nullptr;
}
```
# fs needReadable的判断
取自官方源码注释
```js
// The stream needs another readable event if:
// 1. It is not flowing, as the flow mechanism will take
//    care of it.
// 2. It is not ended.
// 3. It is below the highWaterMark, so we can schedule
//    another readable later.
state.needReadable =
  !state.flowing &&
  !state.ended &&
  state.length <= state.highWaterMark;
```

# stream on data和readable并存
![data_readable](./img/stream_on_event.png)

可以看出，readable和data是可以共存的。

另外也可以看到，data事件触发的点可以总结为，当数据即将丢失的那一刻：
* 如果只有data事件注册，那么当push后，数据不会存在buffer中，此时即可emit data
* 如果同时注册了data和readable事件，当push后，数据将会临时保存在state的buffer中，此时数据是安全的，也不会触发data，只会触发一个readable。当readable的回调执行read()时，此时将会从state buffer中截取已经读取的数据。此时数据将丢失，所以此时也会触发一个data事件。

所以，data事件可以抽象为：当数据离开源和state的buffer，即将丢弃时。

# fs stream总结
> read源码的功能点(388-518)
> * 判断n (388-401)
> * n非0时，emittedReadable设置为false(403-404)
> * 如果n为0，且buffer中数据已满，且有needReadable，则直接emit readable,结束(406-421)
>   * 如果buffer已满，但是n不为0，则再继续读一次
> * 再次计算n，如果为0且ended，触发endReadable(425-430)
> * 判断是否需要doRead  （没有在读取中，且还没有达到highWaterMark）（455-470）
> * 需要执行读取操作。如果state当前为0（一点也没有，则需要设置needReadable;如果有一点，则不用）（475-476）
> * 执行读取操作（478）
> * 获取当前buffer中有的数据，给到ret（486-490）
> * emit data事件

> read(0)的功能：
> 如果state中没有数据，则触发一次_read，从底层读取数据
> 如果state中有数据：
> 1.如果数据已满，则直接触发readable事件，并返回
> 2.如果数据没满，则触发一次_read，已有的数据并不会拿出来（因为n为0）
> 总结为：如果数据满了，则触发readable，否则只是触发_read，从底层读取数据而已。

> read()的功能：
> 如果state中没有数据，则触发一次_read
> 如果state中有数据：
> 1. 如果数据已满，则触发一次_read，并把state中的数据取出来（同时触发data事件）
> 2. 如果数据没满，则还是触发一次_read，并把state中的数据取出来（同时触发data事件）
> 总结为：总会触发一次_read；如果有数据，则取出来。

> read(n)的功能：
> 如果state中没有数据，howMuchToRead返回0，则只会触发一次_read
> 如果state中有数据：
> 1. n小于等于state.length,则触发一次_read，并把state中的数据取出来（同时触发data事件）
> 2. n大于state.length,则还是触发一次_read，但是howMuchToRead返回0，标识数据不足，不再做任何处理
> 总结为：总会触发一次_read；如果有足够（大于等于n）数据，则取出来。

>大总结：
>read一般总会触发底层的_read事件（除非明确指明read(0)，且数据已满）
>非read(0)时的调用，如果state中的数据足够多，则取出来

# fs同步方式可能会导致buffer中的数据超过highWaterMark。
```js
Readable.prototype.read = function(n) {
  ... // 判断n

  ... // 如果read(0)且已满，则直接返回触发readable 

  n = howMuchToRead(n, state);

  // If we've ended, and we're now clear, then finish it up.
  if (n === 0 && state.ended) {
    if (state.length === 0)
      endReadable(this);
    return null;
  }

  

  // 如果此时state中buffer刚刚满，则 state.length - n < state.highWaterMark,所以会继续读
  if (state.length === 0 || state.length - n < state.highWaterMark) {
    doRead = true;
    debug('length less than watermark', doRead);
  }

  if (state.ended || state.reading || state.destroyed || state.errored) {
    doRead = false;
    debug('reading or ended', doRead);
  } else if (doRead) {
    debug('do read');
    state.reading = true;
    state.sync = true;
    // 如果_read是同步模式，那么会继续往state.buffer中push数据，导致state.length超过highWaterMark。
    // 不过没关系，这里只会出现一次，下面将会把state.buffer中的数据取出。
    // 如果n=0，也没关系，因为下一次read，会出现state.length - n < state.highWaterMark 不满足，导致doRead为false，不再读取。
    this._read(state.highWaterMark);
    state.sync = false;
  }

  ...
};
```

# timer使用的双向链表
![双向链表](./img/linkedlist.png)
# net.js和http.js的对比
首先看下样例代码:
1. 通过net.js启动的服务
```js
const net = require('net');
const server = net.createServer((c) => {
  console.log('new connection');// 某个新tcp链接（用户）到来
  c.on('data', () => {
      console.log('data event');
  })
});
```
2. 通过http.js启动的服务
```js
const http = require('http');
const server = http.createServer((req, res) => {
  console.log('new request');// 某个新请求到来
  req.on('data', (data) => {
    console.log('data received');// 请求的数据到来。
  })
});
```
* net.js启动的服务，是以tcp三次握手和真实数据传输为分界点：
  * 三次握手后触发业务回调；
  * 真实数据传输触发c.on('data')回调
* http.js启动的服务，是以头部和body为分界点：
  * 三次握手，设置解析器，解析完头部后，触发业务回调
  * body数据传输后，触发req.on('data')回调

画一张草图，简要表示数据流的区别：
![net/http启动服务区别](./img/netvshttp.png)

# process.nextTick 队列和microtak队列
以下为官方解释（https://nodejs.org/api/）
* process.nextTick() adds callback to the "next tick queue". This queue is fully drained after the current operation on the JavaScript stack runs to completion and before the event loop is allowed to continue. It's possible to create an infinite loop if one were to recursively call process.nextTick(). See the Event Loop guide for more background.
（https://nodejs.org/api/process.html）

* The microtask queue is managed by V8 and may be used in a similar manner to the process.nextTick() queue, which is managed by Node.js. The process.nextTick() queue is always processed before the microtask queue within each turn of the Node.js event loop.

* Removed `process.maxTickDepth`, allowing `process.nextTick` to be used recursively without limit

小结：
* v8维持着一个microtask queue（/deps/src/execution/microtask-queue.cc）
* nodejs维持着一个task_queue（其实是microtask_queue）(/src/node_task_queue.cc + /lib/internal/process/task_queue.js)
* 这里的nextTick的含义，侧重在next tick queue上，即在event loop继续往下走之前（下一个c++/js穿越）之前要清理的队列任务。大多数时候困扰在于，以为是在下一个c++/js穿越后执行。
 
# 多个服务实例
一个nodejs进程，可以运行多个服务实例
const server1 = net.createServer()
const server2 = new.createServer();

server1.listen(8080)
server2.listen(8081)


# node loader
![img](./img/nodeloader.png)

# 文件读写操作和网络请求的区别。
文件读写操作，本质上是对文件创建一个fd进行操作。这个远程请求本质上是一样的，只不过系统把网卡当做fd来处理。
# 一次读取大小
首先，需要解读一下读取前的配置逻辑。
在上一章的2.6小节中，我们知道stream.alloc_cb其实是
```c++
// 文件地址：/src/stream_wrap.cc
[](uv_handle_t* handle, size_t suggested_size, uv_buf_t* buf) {
    static_cast<LibuvStreamWrap*>(handle->data)->OnUvAlloc(suggested_size, buf);
  }

// 文件地址：/deps/uv/src/uv-common.c 
// todo uv_buf_init端点确认细节
uv_buf_t uv_buf_init(char* base, unsigned int len) {
  uv_buf_t buf;
  buf.base = base;
  buf.len = len;
  return buf;
}
```

而在uv__read中，是这样调用它的：
```js
static void uv__read(uv_stream_t* stream) {
    ...
  while (stream->read_cb && (stream->flags & UV_HANDLE_READING) && (count-- > 0)) {
      ...
    stream->alloc_cb((uv_handle_t*)stream, 64 * 1024, &buf);
    ...
    stream->read_cb()
  }
  ...
}
```

可见，这里分配了一个 64 * 1024 = 65536bytes大小的读取量，然后调用读取方法stream->read_cb()进行读取。


# 如何解析req请求
#### 2.2.3 现代框架中如何区分？
现在的框架中，一般使用bodyparser之类的库来解析。我们来看戏koa-bodyparser是怎么做到。

koa-bodyparser
```js
// 文件：npm库koa-bodyparser index.js
var parse = require('co-body');
...
module.exports = function (opts) {
  ...

  return async function bodyParser(ctx, next) {
    ...
        const res = await parseBody(ctx);
    ...
  };

  async function parseBody(ctx) {
    if (enableJson && ((detectJSON && detectJSON(ctx)) || ctx.request.is(jsonTypes))) {
      return await parse.json(ctx, jsonOpts);
    }
    ...
    return {};
  }
};
```
可以看出它依赖了require('co-body')来解析。

```js
// 文件：npm库 co-body
const raw = require('raw-body');
...
// 这里的req就是ctx
module.exports = async function(req, opts) {
  ...
  // 读取headers中的content-length
  let len = req.headers['content-length'];
  const encoding = req.headers['content-encoding'] || 'identity';
  if (len && encoding === 'identity') opts.length = len = ~~len;

  const str = await raw(inflate(req), opts); // inflate 解压缩http数据流
  ...
};
```
这里，co-body通过require('raw-body')，来读取原生的字符串。

玄机就在raw-body这里。
```js
// 文件：npm库 raw-body
// 这里的stream其实还是ctx
function getRawBody (stream, options, callback) {
  
  var length = opts.length != null && !isNaN(opts.length)
    ? parseInt(opts.length, 10)
    : null
  
  if (done) {
    // classic callback style
    return readStream(stream, encoding, length, limit, done)
  }

  return new Promise(function executor (resolve, reject) {
    readStream(stream, encoding, length, limit, function onRead (err, buf) {
      if (err) return reject(err)
      resolve(buf)
    })
  })
}
...
function readStream (stream, encoding, length, limit, callback) {
  ...
  // attach listeners
  stream.on('aborted', onAborted)
  stream.on('close', cleanup)
  stream.on('data', onData)
  stream.on('end', onEnd)
  stream.on('error', onEnd)

  // mark sync section complete
  sync = false

  function done () {
    ...
  }

  function onAborted () {
    ...
  }

  function onData (chunk) {
    ...
  }

  function onEnd (err) {
    ...
  }

  function cleanup () {
    ...
  }
}
```
可以看出，这里通过readStream来读取。读取的大小就在于length。
这个length就是在co-body中，通过let len = req.headers['content-length'];来读取的。
通过Content-Length来判断一个请求的大小。
![alt 如何判定post请求是否结束](./img/postReqCheckEnd.png)

由此来看，对于post类型的请求，我们要寻找的“请求结束标识”，就是请求头中的‘content-lenght’, 即headers['content-length']


一般我们用koa-bodyparser时，都会设置一个大小限制
```js
// 业务代码
app.use(bodyParser({
    formLimit: limitVal,
    jsonLimit: limitVal,
}));

// 文件地址：npm koa-bodyparser

function formatOptions(opts, type) {
  var res = {};
  copy(opts).to(res);
  res.limit = opts[type + 'Limit'];
  return res;
}

// 文件地址：npm co-body
opts.limit = opts.limit || '1mb';

// 文件地址：npm raw-body
 var limit = bytes.parse(opts.limit)
...
function onData (chunk) {
  if (complete) return

  received += chunk.length

  if (limit !== null && received > limit) { // 超过大小限制
    done(createError(413, 'request entity too large', {
      limit: limit,
      received: received,
      type: 'entity.too.large'
    }))
  } else if (decoder) {
    buffer += decoder.write(chunk)
  } else {
    buffer.push(chunk)
  }
}
```